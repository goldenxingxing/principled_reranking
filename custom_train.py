#!/usr/bin/env python
# coding: utf-8

# This file is automatically generated by penrose convert tool
# Feel free to change the configurable code or add your own modules
# You can submit the experiment by running this file directly

import ant_couler.argo as couler

from alps.client import submit_experiment_v2
from alps.util import logger

from alps.framework.experiment import Experiment, TrainConf, RuntimeConf, RestoreConfig, EvalConf
from alps.io.datasetx import DatasetX
from alps.framework.column.column import GroupedSparseColumn, DenseColumn
from alps.framework.train.tensorflow.estimator_builder import EstimatorBuilderV2
from alps.framework.exporter.base import ExportStrategy
from alps.io.reader.odps_reader import OdpsReaderV2
from alps.io.base import OdpsConf
from alps.framework.exporter.base import ExportStrategy, MetricComparator, Goal
import datetime
from aistudio_common.utils import env_utils


def build_experiment(train_table,
                     eval_table,
                     feature_map=None,
                     output_table=None,
                     **kwargs):
    listcnt = 10
    model_name = 'PRM_O3'
    batch_size = 512
    max_steps = 8000000
    epochs = 100
    use_eval = True

    bizdate = (datetime.datetime.strptime(env_utils.get_bizdate(), "%Y%m%d") + datetime.timedelta(days=0)).strftime(
        "%m%d")

    # 1. 特征配置
    # 1.0 用户特征
    features = [
        DenseColumn(
            shape=[1],
            dtype="int64",
            separator=",",
            name='user_id',
            tag='user_id',
            expose_group=False,
            for_serving=True,
        ),
        DenseColumn(
            shape=[8],
            dtype="int64",
            separator=",",
            name='user_sparse',
            tag='user_sparse',
            expose_group=False,
            for_serving=True,
        ),
        DenseColumn(
            shape=[5 * listcnt],
            dtype="int64",
            separator=",",
            name='item_sparse',
            tag='item_sparse',
            expose_group=False,
            for_serving=True,
        ),
        DenseColumn(
            shape=[listcnt],
            dtype="int64",
            separator=",",
            name='pos',
            tag='pos',
            expose_group=False,
            for_serving=True,
        ),
        DenseColumn(
            shape=[listcnt],
            dtype="float",
            separator=",",
            name='item_dense',
            tag='item_dense',
            expose_group=False,
            for_serving=True,
        ),
        DenseColumn(
            shape=[1],
            dtype="int64",
            separator=",",
            name='list_len',
            tag='list_len',
            expose_group=False,
            for_serving=True,
        )
    ]

    labels = [DenseColumn(
        shape=[listcnt],
        dtype="float",
        name="label",
        tag=None,
        expose_group=False,
    )]

    # 2. 训练配置
    train_dataset = DatasetX(
        reader=OdpsReaderV2(
            odps=OdpsConf(
            ),
            table=train_table,
            # partitions=['dt=20220528'],
            max_size=0,
            field_names=None,
            features=features,
            labels=labels,
            enable_aistudio_reader=True,
            aistudio_reader_num_processes=2,
            aistudio_reader_queue_size=512,
            feature_map=None,
            flatten_group=False
        ),
        batch_size=batch_size,
        num_epochs=epochs,
        cache_file=None,
        transformers=[],
        batch_transformers=[],
    )

    train_conf = TrainConf(
        max_steps=max_steps,
        save_checkpoints_steps=5000,
        save_checkpoints_secs=None,
        log_step_count_steps=100,
        save_summary_steps=1000,
        save_timeline_steps=None,
        max_timeline_files=3,
        keep_checkpoint_max=2,
        input=train_dataset,
    )

    # 3. 评估配置
    eval_dataset = DatasetX(
        reader=OdpsReaderV2(
            odps=OdpsConf(
            ),
            table=eval_table,
            # partitions=['dt=20220528'],
            max_size=0,
            field_names=None,
            features=features,
            labels=labels,
            enable_aistudio_reader=True,
            aistudio_reader_num_processes=2,
            aistudio_reader_queue_size=512,
            feature_map=None,
            flatten_group=False
        ),
        batch_size=batch_size,
        num_epochs=1,
        cache_file=None,
        transformers=[],
        batch_transformers=[],
    )

    # EvalConf中steps限制评估进行的步数，配None不限制
    # parallel_validation=True必须使用alps.metrics，参考https://alps.alipay.com/tutorial?inner=alps/vg5vzq/mfq2as
    eval_conf = EvalConf(
        steps=None,
        input=eval_dataset,
        parallel_validation=True,
        throttle_secs=10,
    )

    # 4. 模型配置
    model_builder = EstimatorBuilderV2(
        estimator_name="alps2_sync.custom_model.EmbeddingDNN",
        params={"listcnt": listcnt,
                "lr": 1e-4,
                "l2_reg": 1e-4,
                "batch_size": 32,
                "max_time_len": 10,
                "metric_scope": [1, 3, 5, 10],
                "eb_dim": 16,
                "hidden_size": 64,
                "max_norm": 0,
                'keep_prob': 0.8,
                'model_name': model_name})

    # 5. 导出配置
    from alps.framework.exporter.arks_exporter import ArksExporter

    exporter = ArksExporter(
        # deploy_path=[f'pangu://pangu1_analyze_sata_em14_online/alps/201884/experiment/{model_name}'
        #              ],
        compare=MetricComparator(
            metric="auc",
            goal=Goal.MAXIMIZE,
            threshold=0,
        ),
        export_ckpt=True,
        export_ckpt_to_deploy_path=False,
        strategy=ExportStrategy.BEST,
        as_text=False,
        deploy_path_as_base=True,
        dfs_meta_path=f"dfs://f-b3c6941dqge81.cn-shanghai.dfs.aliyuncs.com:10290/alps/201884/experiment/{model_name}.txt",
        rename_tmp_deploy_path=True,
    )

    # 7. 组装实验
    exp = Experiment(
        train=train_conf,
        eval=eval_conf,
        model_builder=model_builder,
        # runtime=RuntimeConf(
        #     restore_config=[],
        #     representation_config=None,
        #     model_dir=f"pangu://pangu1_analyze_sata_em14_online/alps/201884/experiment/{model_name}/{bizdate}"
        #     # model_dir=f"dfs://f-b3c6941dqge81.cn-shanghai.dfs.aliyuncs.com:10290/alps/201884/tab3_immersion_estimator/{model_name}/{bizdate}"
        # ),
        exporter=exporter,
    )

    return exp


def build_engine(**kwargs):
    from alps.framework.engine import ResourceConf, K8sEngine
    return K8sEngine(
        worker=ResourceConf(
            auto_tune=True
            # num=20,
            # core=20,
            # memory=16210,
            # disk_quota=50000,
        ),
        ps=ResourceConf(
            auto_tune=True
            # num=16,
            # core=15,
            # memory=32120,
        ),
        enable_elastic_ai=True,
        easydl_autoscale=False,
        app='ecocontent',
        image="reg.docker.alibaba-inc.com/alipay-alps/cpu-tf1.13.1-py3:latest",
        # image="reg.docker.alibaba-inc.com/alipay-alps/cpu-tf1.13.1-py3.7:latest",
    )


def add_alps_train_job(step_name, params):
    # 提交，并返回一个job对象
    # 外面可以couler.set_dependencies(lambda: add_alps_train_job(xx))
    exp = build_experiment('pai_temp_512694_15776281_1', 'pai_temp_512694_15776281_1')
    init_rc = \
        """

        ALPS_EXTRA=framework pip install -U alps -i https://pypi.antfin-inc.com/simple/ 
        pip install -U --pre tfplus -i https://pypi.antfin-inc.com/simple/ 

        """
    from alps.framework.engine import LocalEngine
    train_job = submit_experiment_v2(exp,
                                     engine=LocalEngine(),
                                     user="283138",
                                     init_rc=init_rc,
                                     return_job_without_running=True)
    train_job = couler.add_job(train_job, step_name)
    return train_job


if __name__ == "__main__":
    params = {"sub-task": "local-test"}
    couler.set_dependencies(lambda: add_alps_train_job("local-train", params),
                            dependencies=None)
    couler.run(enable_wait=False)
